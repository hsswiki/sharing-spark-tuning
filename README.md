# PySpark Tuning Sharing: AWS EMR As An Example

This repository contains materials related to a technical sharing session I held with my colleagues on PySpark performance tuning.

## Files in this Repository

- `spark_property_calculator.py`: A Python script that helps calculate recommended Spark properties based on cluster configuration, following principles discussed in the sharing.
- `spark_tuning_sharing.pdf`: The slides presented during the technical sharing session.

## Speaking Script

Here is the auto-generated speaking script of mine.

### Spark Properties and Spark Application Architecture

Let's take a look together at the Spark optimization methods we've used in our projects. First up is setting Spark properties. Before diving into specific settings, let's review the architecture of a Spark application and key concepts like node, executor, core, and task. Since everyone in our department is basically a Spark expert, I won't dwell too much on the very basics.

Simply put, when a Spark application runs, it first creates a Spark driver process and then starts executing the user's Spark code. The first part of the Spark code usually involves creating a SparkSession. This SparkSession process communicates with the cluster manager based on the user-defined Spark properties, instructing the cluster manager to create executors on various nodes in the cluster. It's a bit like Xu Jiang's subordinates in "The Knockout" communicating with Xu Jiang, saying his son is in trouble, and then Xu Jiang starts calling people, summoning a bunch of underlings who are ready to cause trouble. The underlings who actually do the work are these executors.

Based on the Spark properties set by the user, the executors will use resources on the nodes. These orange circles, like little oranges, represent cores, or vCores (virtual CPUs) in the case of virtual machines in the cloud. If a Spark property is set so that a Spark executor can use 4 cores, then on a node with 4 cores, there will only be one executor. If it's set so that a Spark executor can only use 2 cores, then on the same node, there will be 2 executors, each using 2 cores. The same applies to memory. For example, if `spark.executor.memory` is set to 16GB, then each executor can only use 16GB of memory, even if the node's memory is much larger than 16GB. It's like the underlings who can't defy their boss's orders and arbitrarily use Xu Jiang's resources.

Finally, regarding the relationship between Task and Core, each core or vCore can process one task in parallel, using one partition. It's a 1:1:1 golden ratio. For example, in this diagram with 2 nodes, a total of 8 little oranges, 8 cores, can process 8 tasks in parallel. These are the relationships between the concepts of node, executor, core, task, and partition.

So, back to our Spark properties. For example, how many cores each executor can use, and how much memory they occupy – these are very important Spark properties that need to be set. They also have some default values, which are described in Spark's official documentation (link was referenced here). However, these default values are likely to be ineffective and may cause the application to fail. It's like buying clothes at Uniqlo – grabbing the first thing you see by the door; you might be able to wear it, but it's likely ill-fitting and uncomfortable.

For instance, the default value for `spark.sql.shuffle.partitions` is 200. This means that during a shuffle operation like a join, the data will be repartitioned into 200 parts. When running a Spark application locally on a laptop, recall our 1:1:1 golden ratio: the number of cores you have determines how many tasks you can process in parallel and how many partitions can be processed concurrently. On this company laptop with 4 cores, it can process 4 tasks in parallel, using 4 partitions. But the default value gives me 200 partitions. This means each core has a queue of 50 tasks (200 divided by 4). Even a workhorse donkey wouldn't agree to that; it's clearly too many. For a Spark application, a task queue length in the single digits is relatively appropriate. Conversely, if I launch an AWS EC2 24XL instance with 96 cores to run a Spark application, 200 tasks would then seem too few.

Of course, running Spark directly on pure EC2 instances isn't ideal. For example, on AWS, you would typically use EMR. EMR is also built on EC2 instances, but Spark, Hadoop, Hive, and other applications are pre-installed on these instances, and multiple instances are grouped into a cluster for you. AWS isn't foolish either; they have adjusted the default Spark property values for different instance sizes, which you can see in this second link I posted (link was referenced here). However, these default values show some improvement, but not much; they are like the 'Sleeping Dragon and Young Phoenix' compared to the original defaults [meaning, not significantly better].

So, when it comes to EMR performance tuning issues, if you contact AWS support, they will definitely ask you, "Have you ever read this blog post?", and then provide a link for you to read. This is the third link I posted (link was referenced here). We've interacted with no less than twenty AWS support personnel, and they all follow this routine. Why? Because this article is genuinely well-written. Especially in the initial phase of a project, by following the method recommended in this article to calculate Spark memory allocation, you are guaranteed to achieve a relatively ideal configuration. It might not be the absolute best configuration for your specific Spark application, but it will be close. As the title of the article suggests, successfully managing memory is the key to whether a Spark application can run smoothly and efficiently. Therefore, I want to go over the main idea of this article with everyone. I have organized its logic into code, which we can look at together.

### Spark Property Helper Program

The main idea of the article is to determine the maximum memory each executor can be allocated by first setting how many cores each executor should use. We start by setting some initial information, such as wanting to use 2 worker nodes and specifying the instance type, for example, an AWS 8XL EC2 instance has 32 cores and 128GB of memory.

Then, here's a very important conclusion, also mentioned in that article: the EMR team found through experiments that using 5 cores per executor is best. Of course, some support personnel we've interacted with have also recommended using 4 cores. We've found that the results are similar whether using 4 or 5 cores.

The next step is to calculate how many executors there are on each node. This is equal to the total number of available cores divided by the number of cores per executor, which is the 4 or 5 we just mentioned. The total number of available cores is equal to the total number of cores on the node minus one. That one core is reserved for Hadoop daemons and other processes like the cluster manager's worker processes.

Next, divide the total memory on the node by the total number of executors to find out the maximum memory each executor can use. However, this memory is divided into two parts: the memory that the executor can actually use, and the memory overhead. According to Spark's default configuration, the memory overhead is 10% of the executor memory. This default value is quite reasonable. So, we can calculate the actual memory each executor can use.

For example, if we run this piece of code with the plan to use 2 8XL worker nodes and keep other settings unchanged, the output will tell us that for the master node, we need to select an instance type with at least 5 cores and 19GB of memory. This is because AWS recommends that the driver's cores and memory should be consistent with the executors. With each executor occupying 5 cores, our calculation shows that it can use a maximum of 19GB of memory. The `spark-submit` section below prints out the formatted common properties like `driver-memory`, `executor-memory`, `driver-cores`, and `executor-cores` for easy copy-pasting when submitting a Spark job.

Finally, we note that the code also calculates the number of tasks that the current cluster can process in parallel. How is this mysterious magic number calculated? Recalling the 1:1:1 golden ratio we mentioned earlier, the number of cores determines how many tasks can be processed in parallel. The number of executor cores in the cluster is equal to the number of worker nodes multiplied by the number of executors per node, multiplied by the number of cores per executor. This way, we get that the current cluster can process 60 tasks in parallel. Where exactly is this mysterious magic number used? Perhaps some attentive listeners remember that it's used to determine whether the number of partitions is reasonable in the context of the default 200 partitions during shuffle. In the following sections on Spark UI and specific code optimization, we will also use this very important calculation result again.

### Spark UI: Jobs

The next part is mainly about how to analyze areas for optimization using the Spark UI after a Spark application has run, then modifying the code or Spark properties, and re-running to observe the continuous improvement in the Spark UI. First, let's review the basic concepts of job, stage, and task.

We mentioned earlier that after a Spark application starts, the Spark driver executes the user-submitted code. The code usually starts with creating a SparkSession, followed by various transformations and actions. We know that Spark operations are divided into two types: transformations and actions. Transformations like `read`, `select`, `filter`, and `join` are lazily evaluated; they only record what you want to do but don't actually operate on the data. They wait until an action is encountered, such as `show`, `count`, or `collect`. At this point, the recorded transformations plus this action are executed together, and the result is returned.

At the task level, each action and the transformations recorded before it are packaged into a job. Within each job, there can be several serial stages. Each stage is separated by a shuffle. Shuffle occurs when some transformations require data to be exchanged between different partitions. For example, when grouping by a certain key, the data for these keys might have been in different partitions before. However, grouping requires that the data for each key within a partition must be the same. In this case, executors work very hard to first exchange data and then perform the operation.

Because stages can only run serially and have dependencies, they cannot run in parallel. Therefore, our optimization goal at the stage level is to minimize shuffles, making the serial parts shorter and the application run faster.

Going down to the next level, within each stage, tasks can run in parallel. Since there is no shuffle within a stage (shuffle happens between stages), the partitions within a stage do not change. At this point, we recall our golden ratio again: 1:1:1, one task uses one core to process one partition. This scenario is a classic example of multi-core parallel computing. A task is also the smallest unit of work in Spark.

Now that we have reviewed the basic concepts of how a job is broken down into stages, and how stages are broken down into tasks, let's go back and see how to examine the Spark UI at each level and what optimizations can be made accordingly.

Next, let's look at the Spark UI screenshot below. It has tabs for Jobs, Stages, etc. The Stages tab also includes information about tasks. First, let's look at the Jobs tab, which is the default view. It will show the total number of jobs in your application. For example, this example has about 200 jobs. According to the experience of AWS support, hundreds of jobs are a bit too many. On the one hand, more jobs lead to management overhead. It's like Xu Jiang assigning tasks to his underlings; if he has over 200 underlings and has to assign tasks and coordinate resources for each one individually, Xu Jiang gets very tired – this is the overhead.

Another reason why too many jobs are not good is that we know one action generates one or more jobs, but the Spark UI only shows the jobs, not which of your actions created these jobs. This is because your code is optimized by Spark before execution, and the order might change. So, for example, if the Spark UI tells you that job number 10 took the longest, you can only roughly guess which action it corresponds to. If there are only a few actions in total, it might be relatively easy to map actions to jobs and then optimize. But if there are over 200 jobs and it tells you that the 100th job has an issue, it becomes very difficult to find the corresponding action.

If you write Spark applications in Scala, it might be a bit better because Spark itself is written in Scala, so it doesn't need to translate your code. The job description will directly hint at the line numbers in the code. Of course, these line numbers are also not completely accurate because it's executing optimized code, but you can look around the hinted line numbers. If you write in PySpark, Spark will ultimately execute translated Java code, and the job description will only hint at Java method names and line numbers. It's difficult to relate the description back to specific PySpark actions just by looking at it. Therefore, when writing PySpark, you should be more mindful of minimizing actions.

So, how specifically can you optimize the number of jobs? One way is during development. For debugging or further development, it's common to frequently use `show`, `count`, `collect`, etc. This is normal. However, once development is complete or the code is relatively stable, remember to remove these unnecessary actions. It's like stewing meat; if you keep lifting the lid to check, the meat won't stew properly. Also, plan out the entire application, roughly outlining the steps, which transformations and actions will be used, and what the inputs, outputs, and dependencies are. Can the process be adjusted to make the transformations and actions used more streamlined?

The image on the right [in the slides] is a rough sketch I made when optimizing our Spark application. The goal was to think about the overall process, their inputs, outputs, and dependencies, and whether the flow could be adjusted to make the transformations and actions more concise. Also, using `cache` at appropriate places can prevent duplicate jobs from being executed multiple times. I will explain `cache` in detail in the code optimization section after covering the Spark UI.

### Spark UI: Stage and Task

After looking at jobs, we can click on the next tab to view the stage and task situation and see what can be optimized. First, at the stage level, we know that stages can only run serially and are separated by shuffles. Therefore, avoid shuffles unless absolutely necessary. For example, when joining, if one table is relatively small and can fit in memory – regarding memory, as we calculated earlier, we have successfully managed memory and are "memory management success stories," knowing how much memory each executor has – then a broadcast join can be used. The smaller table used for joining can be broadcast directly to each executor for local joining, thus avoiding shuffling the larger table.

It's like going to a fruit store to buy some apples, strawberries, oranges, etc. If you pay with cash, you might use some ten-dollar bills, twenty-dollar bills, and one-dollar bills, and the owner gives you a lot of change back. The amount of data shuffled this way is relatively large. Alternatively, you might be very wealthy and just pull out a large bill, saying "keep the change," which saves a lot of fragmented data exchange. In Spark, you can manually broadcast using code, or you can set the `spark.sql.autoBroadcastJoinThreshold` value to automatically broadcast smaller tables for joins. This parameter defaults to 10MB, but for memory management success stories, you can set it larger according to your memory size.

Another shuffle-related optimization is that if a shuffle is unavoidable, you can adjust the order of transformations to reduce the amount of data participating in the shuffle. For example, if you have multiple joins, including `full join`, `left join`, and `inner join`, you can perform the `inner join` first, then the `left join`, and finally the `full join`. This way, the amount of data involved in the shuffle will be relatively small. The preceding `inner join` and `left join` would have already filtered out some data, which won't be carried into the final `full join`. Otherwise, if you perform the `full join` first and then the `left join`, many results from the `full join` might not be used in the end, but the process of shuffling this data would still consume time.

After looking at the stage level, we can delve deeper into the task level, as tasks are the finest-grained units of work, offering more room for optimization. First, let's look at the number of tasks in the current stage. *[This can be seen here in the UI].* For example, this stage has 200 tasks. Since this is a join stage and I haven't changed the `spark.sql.shuffle.partitions` setting (it defaults to 200), it will generate 200 partitions during the shuffle for the join. Each partition corresponds to a task, following the 1:1:1 ratio, hence 200 tasks.

A crucial optimization point is that the number of tasks in a stage should not be less than the number of tasks the current cluster can process in parallel. Remember that mysterious magic number we calculated when determining memory size? That's what we compare it against. In the earlier example, we calculated that the cluster could process 60 tasks in parallel with that configuration. With a total of 200 tasks, each core is essentially handling 3 or 4 tasks. This queue length is relatively reasonable. Thinking back to the initial example of running Spark locally on my company laptop with the same shuffle partition setting, because the number of CPUs is much less than the cluster, each CPU would have a queue of 50 tasks behind it. The overhead would be greater than having a queue of three or four. Of course, 50 is still acceptable, but if you calculate and find thousands or tens of thousands in the queue, it becomes a serious problem. So, having too many tasks compared to the number of tasks that can be processed in parallel is not good, but having too few is also not good because it means some cores are idle. Spark doesn't keep idle workers; this is a situation of insufficient workload.

So, how do we adjust the number of tasks? First, you can adjust the number of partitions using `coalesce` or `repartition`. Fewer partitions mean fewer tasks. However, be aware that `coalesce` directly combines partitions. The advantage is that it involves little to no shuffle, but the disadvantage is that partition sizes may become uneven. For example, if there are currently 3 relatively even partitions, and we want to `coalesce` into 2, it will directly combine two of the partitions into one, leaving the other unchanged. This results in a total of 2 partitions, but one partition is actually twice the size of the other because it was formed from two. In this case, if two tasks process these two partitions in parallel, the task processing the smaller partition will finish sooner, and that core will wait for the other task to finish before both can move to the next stage.

Using `repartition` will make the partitions more even, but it triggers a shuffle. So, there's a trade-off with each method. You need to perform controlled experiments to see which method makes your Spark code run faster.

Another way to control partitions is through the `spark.sql.shuffle.partitions` property. We've already mentioned it twice before. There's a clever trick with this property: you can dynamically change it in your code. For example, before a small join, you might want fewer tasks, so you can set it to a smaller value. Later, before a large join, you can change it back to a larger value. This allows you to keep the number of tasks and partition size within a relatively appropriate state based on the data size at different stages. Of course, doing this is time-consuming and not very cost-effective, so when performance tuning, it's still recommended to focus on areas with high impact and low effort first. This is why we ultimately compiled a checklist, ordered by the project stage and cost-effectiveness, as a reference.

Let's go back to how to adjust the number of tasks. Another relatively simple and convenient method is to adjust the `spark.sql.adaptive.coalescePartitions.parallelismFirst` parameter. This is part of the new AQE feature added in Spark 3.0. Its default is `true`, which favors parallelism and creates more tasks. If you change it to `false`, it will be more restrained and won't create as many partitions, thus resulting in fewer tasks.

After looking at the number of tasks, we can then focus on the task execution time. *[This can be seen in the Duration column].* As mentioned before, partitions might be uneven in size. For example, if one partition is large and another is small, the task processing the large partition might take longer. As shown in the figure [in the slides], you can see that the longest-running task took 1.1 minutes, while the shortest only took 20 seconds. The core for that task will be idle, wasting computing power. If there is data skew and uneven partition sizes, you can use `repartition`, at the cost of a shuffle, to make the partitions more even. This is also a trade-off. How many partitions should you repartition into? That mysterious magic number appears again. Setting the number of partitions to an integer multiple of the number of tasks that can be processed in parallel is a relatively good choice.

Finally, on this screen, we can also pay attention to Garbage Collection and memory spill during task execution. *[This can be seen in these two areas in the UI].* For example, in this stage, many tasks have memory spill and garbage collection, which both consume time. A Spark UI belonging to a memory management success story wouldn't show memory spill. So, how do you successfully manage Spark memory and reach the peak of your career? You see, there are countless problems, but the optimization methods we keep repeating are mainly the same few. For example, don't let partitions get too large, or adjust the executor memory size as mentioned before.

For instance, if each task here spills 5GB of memory, and according to the previous setting, one executor occupies 5 cores and can process 5 tasks simultaneously, $5 \times 5 = 25$, meaning each executor would need to increase its memory by 25GB to prevent spilling. This resource requirement is a bit too large; how big of a cluster would you need to launch? So, in this example, adjusting the partition size is more practical.

Also, if you have cached some tables, remember to `unpersist` them promptly after use; otherwise, they will continue to occupy memory space and can lead to memory spill. So, what to cache, when to cache, and when to unpersist? You can refer to the flowchart mentioned earlier. For example, if our code logic is like this, you can see that this `item level tran data` table in the middle is used by 3 places because there are 3 outgoing lines. The table next to it is also used by 2 places. You can cache these intermediate data that are used multiple times to avoid repeated computations.

A frequently overlooked point is that after caching to memory, you must remember to `unpersist`; otherwise, it will continue to occupy memory space and is prone to causing memory spill.

### Code Level Design Choices

Finally, here are some process design and coding optimization methods you can use. For example, in our project, adjusting the input and output data to a Hadoop partitioned folder structure and using the Parquet file format as much as possible makes it very convenient to split the workload. To handle larger amounts of data, you just need to launch more clusters. This architecture fully utilizes the scalability of the cloud, allowing us to meet customer requirements that the previous team could not.

Regarding partition size, generally, it should be controlled to be around a few dozen MB. This size is a rule of thumb. If it's too large, the workload is not easy to further split. But if it's too small, it might cause excessive network burden during reading, writing, or shuffling. Also, for example, the minimum billing unit for S3 is 128KB, which is like a starting price. If you store a lot of very small files, each one will still be charged according to the starting price, which will also increase project costs.

Another experience regarding utilizing multiple clusters for parallel processing on EMR is that the number of IPs occupied by each cluster is equal to the number of nodes in the cluster plus one. The extra IP is used for internal communication. This has also been confirmed with AWS support. Therefore, when launching a large batch of clusters, you can first estimate whether the remaining IP addresses in the current subnet are sufficient to avoid cluster launch failures due to insufficient remaining IPs, which could lead to data loss.

Another point to note when writing code is to filter as early as possible to avoid shuffling unnecessary data around. After each step, only keep the rows and columns that will be used later. This can reduce the memory burden. Also, try to use APIs like DataFrames, Datasets, and Spark SQL as much as possible, and minimize the use of UDFs. It's like using vector operations in Pandas and avoiding writing loops to calculate the value of each row yourself, because official APIs have significant memory and speed optimizations compared to your own "farm-style" implementations.

Finally, and very importantly, is the use of `cache`. For example, in this piece of code, two tables are joined, and then a new column is added to the result of the join, forming two new tables. How many times is this join executed? On the surface, it looks like once, but theoretically, it will be executed twice. I say theoretically because for such simple scenarios, Spark itself might optimize it, but for complex transformations, Spark cannot cache everything for you. It's best for you to tell it what to cache and when, and when the cache can be cleared.

Going back to this code, the join will be executed twice because the preceding `join`, `withColumn`, etc., are all transformations. Data operations are only triggered when an action is encountered. So, Spark's perspective is looking backward. For example, if you want to know the number of rows in `new_df_1`, `new_df_1` is created by adding a column to `joined_df`, and `joined_df` is created by joining `df1` and `df2`. `df1` and `df2` might be read from some data source. Spark will only truly read the data at this point. Similarly, when calculating the number of rows in `new_df_2`, the `joined_df` table is needed again. To generate this table, `df1` and `df2` will be read again, and then joined again.

However, if we calculate `joined_df` the first time and store it here, the second time we need to use it, we can directly use the cached result. One point that is often overlooked is that after caching to memory, you must remember to `unpersist`; otherwise, it will continue to occupy memory space and is prone to causing memory spill. So, what to cache, when to cache, and when to unpersist? Similarly, you can refer to the flowchart mentioned earlier. For example, if our code logic is like this, you can find that this `item level tran data` table in the middle is used by 3 places because there are 3 outgoing lines. The table next to it is also used by 2 places. You can cache these frequently used intermediate data to avoid repeated computations.

### Checklist

Finally, here is a summary of all the optimization methods mentioned, reordered by the project stage and practicality. I hope it helps everyone. For example, if the project is still in the design phase, you can first design the input and output partition schema, sort out the code logic, and estimate appropriate Spark properties. This will also give you a relatively accurate understanding of the project budget.

Second, during the development phase, or if this project is taking over someone else's existing project where the architecture and process are unlikely to be changed, you can try to filter early, shuffle less, use more APIs, and apply code-level optimizations like caching.

Then, during the testing phase, you can conduct more experiments, adjust Spark properties, and see which settings allow the entire application to run fastest. Also, pay attention to the number of tasks, task duration, partition size, and whether there is memory spill on the Spark UI as guidance for further code optimization.

Finally, there are some other important optimization methods that were not covered in detail because their scope of use may not be as broad as the ones mentioned above, such as AQE, bucketing, and checkpointing. Interested individuals can also search for these topics.
